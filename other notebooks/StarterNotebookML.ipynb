{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd \n",
    "import numpy as np \n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install lightgbm \n",
    "# ! pip install tqdm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_chunks(df, nb_chunks=None, chunksize=None ) :\n",
    "    if bool(nb_chunks) == bool(chunksize):\n",
    "        raise ValueError(\"You must provide only one argument: nb_chunks or chunksize\")\n",
    "    nb_rows = len(df)\n",
    "    if nb_chunks:\n",
    "        chunksize = nb_rows // nb_chunks + (nb_rows % nb_chunks > 0)\n",
    "    for i in range(0, nb_rows, chunksize):\n",
    "        yield df[i : i + chunksize]\n",
    "        \n",
    "def map_amino_acid(x):\n",
    "    return np.array([emb_features_dict[e] for e in x ])\n",
    "def get_features(x): \n",
    "    output=pd.Series()\n",
    "    output[\"mean\"]=np.mean(x)\n",
    "    output[\"min\"]=np.min(x)       \n",
    "    output[\"max\"]=np.max(x)     \n",
    "    output[\"std\"]=np.std(x)      \n",
    "    output[\"var\"]=np.var(x) \n",
    "    output[\"ptp\"]=np.ptp(x) \n",
    "    axis=-1\n",
    "    output[\"min_ax_\"+str(axis)]=np.mean(np.min(x,axis=axis))       \n",
    "    output[\"max_ax_\"+str(axis)]=np.mean(np.max(x,axis=axis))     \n",
    "    output[\"std_ax_\"+str(axis)]=np.mean(np.std(x,axis=axis))      \n",
    "    output[\"var_ax_\"+str(axis)]=np.mean(np.var(x,axis=axis)) \n",
    "    output[\"ptp_ax_\"+str(axis)]= output[\"max_ax_\"+str(axis)]-output[\"min_ax_\"+str(axis)]\n",
    "        \n",
    "    return output\n",
    "\n",
    "def get_features_df_train(chunk):\n",
    "    features=chunk.Sequence.apply(map_amino_acid)\n",
    "    features=features.apply(get_features)\n",
    "    features[\"ID\"]=chunk[\"ID\"]\n",
    "    features[\"target\"]=chunk[\"target\"] \n",
    "    return features\n",
    "\n",
    "def get_features_df_test(chunk):\n",
    "    features=chunk.Sequence.apply(map_amino_acid)\n",
    "    features=features.apply(get_features)\n",
    "    features[\"ID\"]=chunk[\"ID\"]\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_features = pd.read_csv(\"./Zindi/front/amino_acid_embeddings.csv\")\n",
    "emb_features.head()\n",
    "emb_features_dict = {k:np.mean(v) for k,v in zip(emb_features.Amino_Acid,emb_features.drop(\"Amino_Acid\",1).values)}\n",
    "emb_features_dict\n",
    "del emb_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./Zindi/front/train.csv\")\n",
    "test = pd.read_csv(\"./Zindi/front/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunksize = 2000\n",
    "n_jobs = 20 # nbr of CPUs to use (if you increase it you may face memoery issue so try to decrease the chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "228it [02:47,  1.36it/s]\n",
      "57it [00:16,  3.42it/s]\n"
     ]
    }
   ],
   "source": [
    "df_iter = get_df_chunks(train, chunksize=chunksize)\n",
    "train_features = Parallel(n_jobs=n_jobs)(delayed(get_features_df_train)(chunk) for chunk in tqdm(df_iter))\n",
    "df_iter = get_df_chunks(test, chunksize=chunksize)\n",
    "test_features = Parallel(n_jobs=n_jobs)(delayed(get_features_df_test)(chunk) for chunk in tqdm(df_iter))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = pd.concat(test_features)\n",
    "train_features = pd.concat(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features.to_csv(\"./proc_data/test_features.csv\",index=False)\n",
    "train_features.to_csv(\"./proc_data/train_features.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,val = train_test_split(train_features,test_size=0.1,random_state=1994)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "params={'bagging_fraction': 1,\n",
    " 'bagging_freq': 1,\n",
    " 'boosting_type': 'gbdt',\n",
    " 'feature_fraction': 0.4,\n",
    " 'learning_rate': 0.01,\n",
    " 'max_depth': 7,\n",
    " 'metric': 'multi_logloss',\n",
    " 'min_data_in_leaf': 33,\n",
    " 'num_leaves': 44,\n",
    " 'num_threads': 8,\n",
    " 'objective': 'multiclass',\n",
    " \"num_class\":train.target.nunique(),\n",
    " 'seed': 2020,\n",
    " 'tree_learner': 'serial'}\n",
    "num_rounds=1000000\n",
    "early_stoping_rounds=50\n",
    "verbose_eval=50\n",
    "features=train.drop([\"ID\",\"target\"],1).columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgbm_model(train,validation,test_data,features,target_name,params):\n",
    "    dtrain = lgbm.Dataset(train[features],train[target_name])\n",
    "    dval = lgbm.Dataset(validation[features],validation[target_name])\n",
    "    lgbm_model= lgbm.train(params=params,\n",
    "                train_set=dtrain,\n",
    "                num_boost_round=num_rounds,\n",
    "                valid_sets=[dtrain,dval],\n",
    "                 verbose_eval=verbose_eval,\n",
    "                 early_stopping_rounds=early_stoping_rounds)\n",
    "    best_iteration = lgbm_model.best_iteration\n",
    "    validation_prediction=lgbm_model.predict(validation[features], num_iteration=best_iteration)\n",
    "    test_prediction=lgbm_model.predict(test_data[features], num_iteration=best_iteration)\n",
    "    return test_prediction ,validation_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "[50]\ttraining's multi_logloss: 0.911596\tvalid_1's multi_logloss: 0.912069\n",
      "[100]\ttraining's multi_logloss: 0.863123\tvalid_1's multi_logloss: 0.864178\n",
      "[150]\ttraining's multi_logloss: 0.830941\tvalid_1's multi_logloss: 0.832548\n",
      "[200]\ttraining's multi_logloss: 0.809094\tvalid_1's multi_logloss: 0.811154\n",
      "[250]\ttraining's multi_logloss: 0.793007\tvalid_1's multi_logloss: 0.795504\n",
      "[300]\ttraining's multi_logloss: 0.779926\tvalid_1's multi_logloss: 0.782905\n",
      "[350]\ttraining's multi_logloss: 0.770791\tvalid_1's multi_logloss: 0.77421\n",
      "[400]\ttraining's multi_logloss: 0.764042\tvalid_1's multi_logloss: 0.767841\n",
      "[450]\ttraining's multi_logloss: 0.758619\tvalid_1's multi_logloss: 0.762795\n",
      "[500]\ttraining's multi_logloss: 0.754348\tvalid_1's multi_logloss: 0.758899\n",
      "[550]\ttraining's multi_logloss: 0.750776\tvalid_1's multi_logloss: 0.755683\n",
      "[600]\ttraining's multi_logloss: 0.747798\tvalid_1's multi_logloss: 0.75307\n",
      "[650]\ttraining's multi_logloss: 0.745248\tvalid_1's multi_logloss: 0.750841\n",
      "[700]\ttraining's multi_logloss: 0.742967\tvalid_1's multi_logloss: 0.74888\n",
      "[750]\ttraining's multi_logloss: 0.741203\tvalid_1's multi_logloss: 0.747434\n",
      "[800]\ttraining's multi_logloss: 0.739577\tvalid_1's multi_logloss: 0.746103\n",
      "[850]\ttraining's multi_logloss: 0.738496\tvalid_1's multi_logloss: 0.745292\n",
      "[900]\ttraining's multi_logloss: 0.73724\tvalid_1's multi_logloss: 0.744303\n",
      "[950]\ttraining's multi_logloss: 0.73614\tvalid_1's multi_logloss: 0.743527\n",
      "[1000]\ttraining's multi_logloss: 0.735315\tvalid_1's multi_logloss: 0.742974\n",
      "[1050]\ttraining's multi_logloss: 0.734401\tvalid_1's multi_logloss: 0.742341\n",
      "[1100]\ttraining's multi_logloss: 0.733709\tvalid_1's multi_logloss: 0.741895\n",
      "[1150]\ttraining's multi_logloss: 0.733048\tvalid_1's multi_logloss: 0.741463\n",
      "[1200]\ttraining's multi_logloss: 0.732462\tvalid_1's multi_logloss: 0.741128\n",
      "[1250]\ttraining's multi_logloss: 0.731932\tvalid_1's multi_logloss: 0.74083\n",
      "[1300]\ttraining's multi_logloss: 0.731429\tvalid_1's multi_logloss: 0.740548\n",
      "[1350]\ttraining's multi_logloss: 0.730963\tvalid_1's multi_logloss: 0.740304\n",
      "[1400]\ttraining's multi_logloss: 0.730558\tvalid_1's multi_logloss: 0.740097\n",
      "[1450]\ttraining's multi_logloss: 0.730145\tvalid_1's multi_logloss: 0.739912\n",
      "[1500]\ttraining's multi_logloss: 0.729786\tvalid_1's multi_logloss: 0.739757\n",
      "[1550]\ttraining's multi_logloss: 0.729448\tvalid_1's multi_logloss: 0.739598\n",
      "[1600]\ttraining's multi_logloss: 0.72916\tvalid_1's multi_logloss: 0.739497\n",
      "[1650]\ttraining's multi_logloss: 0.728842\tvalid_1's multi_logloss: 0.739373\n",
      "[1700]\ttraining's multi_logloss: 0.728537\tvalid_1's multi_logloss: 0.73927\n",
      "[1750]\ttraining's multi_logloss: 0.728276\tvalid_1's multi_logloss: 0.739199\n",
      "[1800]\ttraining's multi_logloss: 0.727972\tvalid_1's multi_logloss: 0.739118\n",
      "[1850]\ttraining's multi_logloss: 0.727719\tvalid_1's multi_logloss: 0.739074\n",
      "[1900]\ttraining's multi_logloss: 0.72743\tvalid_1's multi_logloss: 0.739012\n",
      "[1950]\ttraining's multi_logloss: 0.727192\tvalid_1's multi_logloss: 0.73897\n",
      "[2000]\ttraining's multi_logloss: 0.726991\tvalid_1's multi_logloss: 0.738937\n",
      "[2050]\ttraining's multi_logloss: 0.726807\tvalid_1's multi_logloss: 0.738893\n",
      "[2100]\ttraining's multi_logloss: 0.726595\tvalid_1's multi_logloss: 0.738858\n",
      "[2150]\ttraining's multi_logloss: 0.726424\tvalid_1's multi_logloss: 0.738845\n",
      "[2200]\ttraining's multi_logloss: 0.726253\tvalid_1's multi_logloss: 0.738839\n",
      "[2250]\ttraining's multi_logloss: 0.726099\tvalid_1's multi_logloss: 0.738836\n",
      "[2300]\ttraining's multi_logloss: 0.725943\tvalid_1's multi_logloss: 0.738833\n",
      "Early stopping, best iteration is:\n",
      "[2295]\ttraining's multi_logloss: 0.725957\tvalid_1's multi_logloss: 0.73883\n"
     ]
    }
   ],
   "source": [
    "test_pred,val_pred = lgbm_model(train, val, test_features, features, \"target\", params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = test[[\"ID\"]].copy()\n",
    "for i in range(test_pred.shape[1]):\n",
    "    sub[\"target_{}\".format(i)]=test_pred[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"StarterNotebookML_sub.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
