{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as  pd \n",
    "import numpy as np \n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import os \n",
    "from sklearn.model_selection import train_test_split\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\" # change it to \"0\" if yo have only one gpu or the gpu numbe  that you would like to use "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_txt(file_name,column):\n",
    "    with open(file_name, 'w') as f:\n",
    "        for item in column:\n",
    "            f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv(\"./Zindi/front/train.csv\")\n",
    "test=pd.read_csv(\"./Zindi/front/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length=550# max seq length in this data set is 550 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data to train and validation \n",
    "train,val=train_test_split(train,test_size=0.1,random_state=1994)\n",
    "\n",
    "#reduce seq length\n",
    "if max_seq_length>550 : \n",
    "    train[\"Sequence\"]=train[\"Sequence\"].apply(lambda x: \"\".join(list(x)[0:max_seq_length]))\n",
    "    val[\"Sequence\"]=val[\"Sequence\"].apply(lambda x: \"\".join(list(x)[0:max_seq_length]))\n",
    "    test[\"Sequence\"]=test[\"Sequence\"].apply(lambda x: \"\".join(list(x)[0:max_seq_length]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write Sequnce column to txt file \n",
    "write_to_txt(\"proc_data/train.txt\",train.Sequence)\n",
    "write_to_txt(\"proc_data/test.txt\",test.Sequence)\n",
    "write_to_txt(\"proc_data/val.txt\",val.Sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=train[[\"target\"]].copy()\n",
    "val_label=val[[\"target\"]].copy()\n",
    "train_label.to_csv(\"./proc_data/train_label.csv\",index=False)\n",
    "val_label.to_csv(\"./proc_data/val_label.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label=pd.read_csv(\"./proc_data/train_label.csv\")\n",
    "val_label=pd.read_csv(\"./proc_data/val_label.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size=512\n",
    "val_batch_size=512\n",
    "number_of_class=train_label.target.nunique()\n",
    "train_steps = len(train_label) // train_batch_size + int(len(train_label) % train_batch_size > 0)\n",
    "val_steps = len(val_label) // val_batch_size + int(len(val_label) % val_batch_size > 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voc_set=set(['P', 'V', 'I', 'K', 'N', 'B', 'F', 'Y', 'E', 'W', 'R', 'D', 'X', 'S', 'C', 'U', 'Q', 'A', 'M', 'H', 'L', 'G', 'T'])\n",
    "voc_set_map={ k:v for k , v in zip(voc_set,range(1,len(voc_set)+1))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text_tensor, label):\n",
    "    encoded_text = [ voc_set_map[e] for e in list(text_tensor.numpy().decode())]\n",
    "    return encoded_text, label\n",
    "def encode_map_fn(text, label):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text, label = tf.py_function(encode, \n",
    "                                       inp=[text, label], \n",
    "                                       Tout=(tf.int64, tf.int64))\n",
    "    encoded_text.set_shape([None])\n",
    "    label=tf.one_hot(label,number_of_class)\n",
    "    label.set_shape([number_of_class])\n",
    "    \n",
    "    return encoded_text, label\n",
    "def get_data_loader(file,batch_size,labels):\n",
    "    \n",
    "    label_data=tf.data.Dataset.from_tensor_slices(labels.target)\n",
    "    data_set=tf.data.TextLineDataset(file)\n",
    "    data_set=tf.data.Dataset.zip((data_set,label_data))\n",
    "\n",
    "    data_set=data_set.repeat()\n",
    "    data_set = data_set.shuffle(len(labels))\n",
    "    data_set=data_set.map(encode_map_fn,tf.data.experimental.AUTOTUNE)\n",
    "    data_set=data_set.padded_batch(batch_size)\n",
    "    data_set = data_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data_set\n",
    "\n",
    "\n",
    "def get_data_loader_test(file,batch_size,labels):\n",
    "    \n",
    "    label_data=tf.data.Dataset.from_tensor_slices(labels.target)\n",
    "    data_set=tf.data.TextLineDataset(file)\n",
    "    data_set=tf.data.Dataset.zip((data_set,label_data))\n",
    "    data_set=data_set.map(encode_map_fn,tf.data.experimental.AUTOTUNE)\n",
    "    data_set=data_set.padded_batch(batch_size)\n",
    "    data_set = data_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl=get_data_loader(\"proc_data/train.txt\",train_batch_size,train_label)\n",
    "val_dl=get_data_loader(\"proc_data/val.txt\",train_batch_size,val_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input,Dense,Dropout,Embedding,Concatenate,Flatten,LSTM ,Bidirectional\n",
    "from tensorflow.keras.activations import relu ,sigmoid,softmax\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "def model():\n",
    "    name=\"seq\"\n",
    "    dropout_rate=0.1\n",
    "    learning_rate=0.001\n",
    "    sequnce=Input([None],name=\"sequnce\")\n",
    "    \n",
    "    EMB_layer=Embedding(input_dim=len(voc_set)+1,output_dim=64,name=\"emb_layer\")\n",
    "    \n",
    "\n",
    "    LSTM_layer_2=LSTM(units=256,name=\"lstm_2\",return_sequences=False)\n",
    "    BIDIR_layer_2=Bidirectional(LSTM_layer_2,name=\"bidirectional_2\")\n",
    "    \n",
    "    Dens_layer_1=Dense(units=512,activation=relu,kernel_regularizer=None,bias_regularizer=None,name=name+\"_dense_layer_1\")\n",
    "    Dens_layer_2=Dense(units=256,activation=relu,kernel_regularizer=None,bias_regularizer=None,name=name+\"_dense_layer_2\")\n",
    "    \n",
    "    output=Dense(units=number_of_class,activation=softmax,kernel_regularizer=None,bias_regularizer=None,name=name+\"_dense_layer_output\")\n",
    "    \n",
    "    dropout_1=Dropout(dropout_rate)\n",
    "    \n",
    "    \n",
    "    emb_layer=EMB_layer(sequnce)\n",
    "    logits=output(Dens_layer_2(dropout_1(Dens_layer_1(BIDIR_layer_2(emb_layer)))))\n",
    "\n",
    "    \n",
    "    model=tf.keras.Model(inputs={\"sequnce\":sequnce, },outputs=logits) \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss=CategoricalCrossentropy(), metrics=[tf.keras.metrics.CategoricalAccuracy(name=\"Acc\")]) \n",
    "    model.summary()\n",
    "    return model \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can add eraly stoping method as callback and save best  model to improve your score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dl,\n",
    "                    validation_data=val_dl,\n",
    "                    epochs=5,\n",
    "                    verbose=1,\n",
    "                    validation_steps=val_steps,\n",
    "                    steps_per_epoch=train_steps\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_test(text_tensor):\n",
    "    encoded_text = [ voc_set_map[e] for e in list(text_tensor.numpy().decode())]\n",
    "    return (encoded_text)\n",
    "def encode_map_fn_test(text):\n",
    "    # py_func doesn't set the shape of the returned tensors.\n",
    "    encoded_text = tf.py_function(encode_test, \n",
    "                                       inp=[text], \n",
    "                                       Tout=tf.int64)\n",
    "    encoded_text.set_shape([None])\n",
    "\n",
    "    \n",
    "    return (encoded_text)\n",
    "\n",
    "def get_test_data_loader(file,batch_size):\n",
    "    data_set=tf.data.TextLineDataset(file)\n",
    "    data_set=data_set.map(encode_map_fn_test,tf.data.experimental.AUTOTUNE)\n",
    "    data_set=data_set.padded_batch(batch_size)\n",
    "    data_set = data_set.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test=pd.read_csv(\"./Zindi/front/test.csv\")\n",
    "test[\"target\"]=0\n",
    "test_dl=get_data_loader_test(\"proc_data/test.txt\",512,test)\n",
    "test_pred=model.predict(test_dl,verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub=test[[\"ID\"]].copy()\n",
    "for i in range(number_of_class):\n",
    "    sub[\"target_{}\".format(i)]=test_pred[:,i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(\"sub.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
